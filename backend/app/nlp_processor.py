import pandas as pd
import numpy as np
import asyncio
from typing import Dict, List, Any, Tuple
import warnings
import re
import os
import json
import aiohttp
from collections import Counter
from datetime import datetime

# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö pythainlp (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
try:
    from pythainlp.tokenize import word_tokenize
    from pythainlp.tag import pos_tag
    from pythainlp.classify import GzipModel
    PYTHAINLP_AVAILABLE = True
    print("‚úÖ PyThaiNLP imported successfully")
except ImportError as e:
    PYTHAINLP_AVAILABLE = False
    print(f"‚ö†Ô∏è PyThaiNLP not available: {e}")
    print("Will use basic tokenization instead")

warnings.filterwarnings('ignore')

class NLPProcessor:
    def __init__(self, analysis_method: int = 0):
        """
        Initialize NLP Processor
        
        Args:
            analysis_method (int): 
                0 = Rule-based analysis (default)
                1 = GzipModel trained analysis
                2 = SSense API analysis (AI for Thai)
        """
        self.models_path = "./app/data/models"
        self.training_data_path = "./app/data/training/"
        self._ready = False
        
        # Analysis method selection
        self.analysis_method = analysis_method
        self.available_methods = {
            0: {"name": "Rule-based", "function": "_analyze_sentiment_rule_based"},
            1: {"name": "GzipModel", "function": "_analyze_sentiment_gzip_model"},
            2: {"name": "SSense API", "function": "_analyze_sentiment_ssense"},
        }
        
        # Trained model ‡∏à‡∏≤‡∏Å GzipModel (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö method = 1)
        self.trained_model = None
        self.model_trained = False
        
        # SSense API configuration (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö method = 2)
        self.ssense_api_key = "5Rx7TLNeMdtYyhub2A74VJ3HVLThqWRk"
        self.ssense_url = "https://api.aiforthai.in.th/ssense"
        
        # ‡∏Ñ‡∏≥‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö survey ‡∏ô‡∏µ‡πâ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö fallback)
        self.positive_indicators = {
            '‡∏î‡∏µ', '‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°', '‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î', '‡∏ä‡∏≠‡∏ö', '‡πÄ‡∏ó‡∏û', '‡πÄ‡∏à‡πã‡∏á', '‡∏™‡∏∞‡∏î‡∏ß‡∏Å', '‡∏á‡πà‡∏≤‡∏¢', 
            '‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß', '‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à', '‡∏û‡∏≠‡πÉ‡∏à', '‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô', '‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢', '‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢',
            '‡∏™‡∏ö‡∏≤‡∏¢', '‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô', '‡∏ñ‡∏π‡∏Å‡πÉ‡∏à', '‡∏ô‡πà‡∏≤‡πÉ‡∏ä‡πâ', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì', 'perfect',
            'good', 'nice', 'great', 'excellent', '‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥', '‡∏ä‡∏∑‡πà‡∏ô‡∏ä‡∏°', '‡∏¢‡∏≠‡∏î‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°',
            '‡∏Ñ‡∏∏‡πâ‡∏ô‡πÄ‡∏Ñ‡∏¢‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£', '‡∏≠‡∏¥‡πÄ‡∏•‡πá‡∏Å‡∏ó‡∏£‡∏≠‡∏ô‡∏¥‡∏Å', '‡∏ñ‡∏ô‡∏±‡∏î', '‡πÑ‡∏°‡πà‡∏¢‡∏≤‡∏Å',
            '‡πÑ‡∏°‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô', '‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡πâ‡∏≠‡∏¢', '‡πÄ‡∏£‡πá‡∏ß', '‡∏ó‡∏±‡∏ô‡πÉ‡∏à', '‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå', '‡πÇ‡∏≠‡πÄ‡∏Ñ', 'ok',
            '‡πÑ‡∏î‡πâ', '‡∏õ‡∏Å‡∏ï‡∏¥', '‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°', '‡∏û‡∏≠‡∏î‡∏µ', '‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°', '‡πÑ‡∏°‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô', '‡∏™‡∏ô‡πÉ‡∏à'
        }
        
        self.negative_indicators = {
            '‡πÅ‡∏¢‡πà', '‡πÑ‡∏°‡πà‡∏î‡∏µ', '‡∏ä‡πâ‡∏≤', '‡∏¢‡∏≤‡∏Å', '‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô', '‡∏™‡∏±‡∏ö‡∏™‡∏ô', '‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à', '‡∏õ‡∏±‡∏ç‡∏´‡∏≤',
            '‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î', '‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á', '‡∏ö‡∏±‡πä‡∏Å', 'error', '‡πÑ‡∏°‡πà‡∏™‡∏∞‡∏î‡∏ß‡∏Å', '‡∏¢‡∏∏‡πà‡∏á‡∏¢‡∏≤‡∏Å', '‡∏•‡∏≥‡∏ö‡∏≤‡∏Å',
            '‡∏´‡∏á‡∏∏‡∏î‡∏´‡∏á‡∏¥‡∏î', '‡∏£‡∏≥‡∏Ñ‡∏≤‡∏ç', '‡πÑ‡∏°‡πà‡∏ä‡∏≠‡∏ö', '‡πÑ‡∏°‡πà‡∏û‡∏≠‡πÉ‡∏à', '‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á', '‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥',
            'bad', 'terrible', 'horrible', '‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠', '‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á', '‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞',
            '‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á', '‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç', '‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°', '‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö', '‡πÑ‡∏°‡πà‡∏°‡∏µ', '‡∏´‡∏≤‡∏¢',
            '‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢', '‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô', '‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô', '‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö', '‡∏Ç‡∏≤‡∏î', '‡∏ô‡πâ‡∏≠‡∏¢',
            '‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠', '‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô', '‡∏ú‡∏¥‡∏î', '‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ', '‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à'
        }
        
        # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏≤‡∏á
        self.neutral_indicators = {
            '‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£', '‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß', '‡∏õ‡∏Å‡∏ï‡∏¥', '‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤', '‡πÄ‡∏â‡∏¢‡πÜ', '‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à', 
            '‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ', '‡∏≠‡∏≤‡∏à‡∏à‡∏∞', '‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á', '‡∏û‡∏≠‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ', '‡∏Å‡πá‡πÑ‡∏î‡πâ', 'record',
            '‡∏ó‡∏î‡∏™‡∏≠‡∏ö', '‡∏£‡∏∞‡∏ö‡∏ö', '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•', '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö', '‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà',
            '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£', '‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°', '‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ'
        }
        
        # ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á survey
        self.survey_keywords = {
            '‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö': ['‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö', '‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô', 'login', 'otp', 'email', 'tha id'],
            '‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô': ['‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô', '‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', '‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô', '‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô'],
            '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•': ['‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡∏ï‡∏£‡∏≤‡∏™‡∏≤‡∏£‡∏´‡∏ô‡∏µ‡πâ', '‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢', '‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î'],
            '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà': ['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏õ‡∏µ', '‡∏û‡∏®', '‡∏Ñ‡∏®', 'format', '‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö'],
            '‡∏õ‡∏∏‡πà‡∏°': ['‡∏õ‡∏∏‡πà‡∏°', 'button', 'edit', '‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç', '‡∏Å‡∏î', '‡∏Ñ‡∏•‡∏¥‡∏Å'],
            '‡∏†‡∏≤‡∏©‡∏≤': ['‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏Ñ‡∏≥', '‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°', '‡∏®‡∏±‡∏û‡∏ó‡πå'],
            '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£': ['‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£', '‡∏ò‡∏õ‡∏ó', '‡∏™‡∏≤‡∏Ç‡∏≤', '‡∏°‡∏≤‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£'],
            '‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠': ['‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠', '‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•', '‡πÄ‡∏°‡∏ô‡∏π', '‡∏´‡∏ô‡πâ‡∏≤', 'screen'],
            '‡πÄ‡∏ß‡∏•‡∏≤': ['‡πÄ‡∏ß‡∏•‡∏≤', '‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', '‡∏ô‡∏≤‡∏ô', '‡πÄ‡∏£‡πá‡∏ß', '‡∏ä‡πâ‡∏≤', '‡∏£‡∏≠']
        }
        
    async def initialize(self):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏∞‡∏ö‡∏ö NLP"""
        try:
            print("üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏∞‡∏ö‡∏ö Survey NLP Analysis...")
            print(f"üéØ Analysis Method: {self.analysis_method} ({self.available_methods[self.analysis_method]['name']})")
            
            os.makedirs(self.models_path, exist_ok=True)
            os.makedirs(self.training_data_path, exist_ok=True)
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö PyThaiNLP ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            if PYTHAINLP_AVAILABLE:
                test_text = "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏î‡∏µ‡∏à‡∏±‡∏á"
                try:
                    tokens = word_tokenize(test_text)
                    print(f"‚úÖ PyThaiNLP ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
                    
                    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≤‡∏° method ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
                    await self._initialize_method()
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è PyThaiNLP error: {e}")
                    if self.analysis_method == 1:
                        print("‚ö†Ô∏è GzipModel requires PyThaiNLP, switching to Rule-based")
                        self.analysis_method = 0
            else:
                print("‚ö†Ô∏è PyThaiNLP ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
                if self.analysis_method == 1:
                    print("‚ö†Ô∏è GzipModel requires PyThaiNLP, switching to Rule-based")
                    self.analysis_method = 0
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö SSense API ‡∏ñ‡πâ‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å method 2
            if self.analysis_method == 2:
                await self._test_ssense_api()
            
            self._ready = True
            method_name = self.available_methods[self.analysis_method]['name']
            print(f"‚úÖ Survey NLP Processor ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ({method_name})")
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
            self._ready = True  # ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
    
    async def _initialize_method(self):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≤‡∏° analysis method ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å"""
        if self.analysis_method == 0:
            # Rule-based ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏≠‡∏∞‡πÑ‡∏£
            print("üìã Rule-based analysis ready")
            
        elif self.analysis_method == 1:
            # GzipModel ‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
            await self._load_and_train_gzip_model()
            
        elif self.analysis_method == 2:
            # SSense API ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
            print("üåê Testing SSense API connection...")
    
    async def _test_ssense_api(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ SSense API"""
        try:
            test_text = "‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö"
            headers = {
                'Apikey': self.ssense_api_key
            }
            params = {'text': test_text}
            
            async with aiohttp.ClientSession() as session:
                async with session.get(self.ssense_url, headers=headers, params=params, timeout=10) as response:
                    if response.status == 200:
                        result = await response.json()
                        print("‚úÖ SSense API ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
                        return True
                    else:
                        print(f"‚ö†Ô∏è SSense API error: {response.status}")
                        print("üîÑ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Rule-based ‡πÅ‡∏ó‡∏ô")
                        self.analysis_method = 0
                        return False
        except Exception as e:
            print(f"‚ö†Ô∏è SSense API connection failed: {e}")
            print("üîÑ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Rule-based ‡πÅ‡∏ó‡∏ô")
            self.analysis_method = 0
            return False
    
    def set_analysis_method(self, method: int) -> bool:
        """‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô analysis method"""
        if method in self.available_methods:
            self.analysis_method = method
            print(f"üîÑ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô {self.available_methods[method]['name']}")
            return True
        else:
            print(f"‚ùå Invalid method {method}. Available: {list(self.available_methods.keys())}")
            return False
    
    def get_available_methods(self) -> Dict[int, Dict[str, Any]]:
        """‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ analysis methods ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ"""
        return self.available_methods.copy()
    
    async def _load_and_train_gzip_model(self):
        """‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• GzipModel (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö method = 1)"""
        try:
            print("ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô GzipModel...")
            print(f"üìÇ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡πÉ‡∏ô: {self.training_data_path}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not os.path.exists(self.training_data_path):
                print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {self.training_data_path}")
                # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÉ‡∏ô paths ‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
                alternative_paths = [
                    "/app/data/training",
                    "./data/training", 
                    "data/training",
                    "/data/training"
                ]
                
                for alt_path in alternative_paths:
                    if os.path.exists(alt_path):
                        print(f"‚úÖ ‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÉ‡∏ô: {alt_path}")
                        self.training_data_path = alt_path
                        break
                else:
                    print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå data/training ‡πÉ‡∏ô paths ‡πÉ‡∏î‡πÜ")
                    print("üìÅ ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô:")
                    try:
                        current_files = os.listdir(".")
                        for f in current_files:
                            print(f"  - {f}")
                            if os.path.isdir(f) and f == "data":
                                data_files = os.listdir(f)
                                for df in data_files:
                                    print(f"    - {f}/{df}")
                                    if os.path.isdir(os.path.join(f, df)) and df == "training":
                                        training_files = os.listdir(os.path.join(f, df))
                                        for tf in training_files:
                                            file_path = os.path.join(f, df, tf)
                                            if os.path.isfile(file_path):
                                                try:
                                                    with open(file_path, 'r', encoding='utf-8') as temp_f:
                                                        lines = temp_f.readlines()
                                                        print(f"      - {tf}: {len(lines)} ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î")
                                                        # ‡πÅ‡∏™‡∏î‡∏á 3 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å
                                                        for i, line in enumerate(lines[:3]):
                                                            print(f"        {i+1}: {line.strip()}")
                                                except Exception as e:
                                                    print(f"      - {tf}: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ ({e})")
                    except Exception as e:
                        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå: {e}")
            
            training_data = []
            files_info = {}
            
            # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• positive
            pos_file = os.path.join(self.training_data_path, "pos.txt")
            print(f"üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå: {pos_file}")
            if os.path.exists(pos_file):
                try:
                    with open(pos_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        files_info['pos_total_lines'] = len(lines)
                        valid_lines = 0
                        for line_num, line in enumerate(lines, 1):
                            line = line.strip()
                            if line:
                                training_data.append((line, "positive"))
                                valid_lines += 1
                            else:
                                print(f"    ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î {line_num}: ‡∏ß‡πà‡∏≤‡∏á")
                        print(f"üìù ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• positive: {valid_lines}/{len(lines)} ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ (‡∏Ç‡πâ‡∏≤‡∏° {len(lines)-valid_lines} ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á)")
                        
                        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                        print("    ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• positive:")
                        pos_data = [x[0] for x in training_data if x[1] == "positive"]
                        for i, example in enumerate(pos_data[:3]):
                            print(f"      {i+1}: {example}")
                            
                except Exception as e:
                    print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå pos.txt: {e}")
            else:
                print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {pos_file}")
            
            # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• negative
            neg_file = os.path.join(self.training_data_path, "neg.txt")
            print(f"üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå: {neg_file}")
            if os.path.exists(neg_file):
                try:
                    with open(neg_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        files_info['neg_total_lines'] = len(lines)
                        valid_lines = 0
                        for line_num, line in enumerate(lines, 1):
                            line = line.strip()
                            if line:
                                training_data.append((line, "negative"))
                                valid_lines += 1
                        print(f"üìù ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• negative: {valid_lines}/{len(lines)} ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ")
                        
                        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                        print("    ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• negative:")
                        neg_data = [x[0] for x in training_data if x[1] == "negative"]
                        for i, example in enumerate(neg_data[:3]):
                            print(f"      {i+1}: {example}")
                            
                except Exception as e:
                    print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå neg.txt: {e}")
            else:
                print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {neg_file}")
            
            # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• neutral
            neu_file = os.path.join(self.training_data_path, "neu.txt")
            print(f"üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå: {neu_file}")
            if os.path.exists(neu_file):
                try:
                    with open(neu_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        files_info['neu_total_lines'] = len(lines)
                        valid_lines = 0
                        for line_num, line in enumerate(lines, 1):
                            line = line.strip()
                            if line:
                                training_data.append((line, "neutral"))
                                valid_lines += 1
                        print(f"üìù ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• neutral: {valid_lines}/{len(lines)} ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ")
                        
                        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                        print("    ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• neutral:")
                        neu_data = [x[0] for x in training_data if x[1] == "neutral"]
                        for i, example in enumerate(neu_data[:3]):
                            print(f"      {i+1}: {example}")
                            
                except Exception as e:
                    print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå neu.txt: {e}")
            else:
                print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {neu_file}")
            
            print(f"üìä ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(training_data)} ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ")
            print(f"üìä ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏ü‡∏•‡πå: {files_info}")
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å
            if len(training_data) == 0:
                print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏•‡∏¢ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á...")
                training_data = await self._create_sample_training_data()
            elif len(training_data) < 50 and len(training_data) > 0:
                print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏µ‡πÄ‡∏û‡∏µ‡∏¢‡∏á {len(training_data)} ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á...")
                # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏ß‡πâ ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
                real_data = training_data.copy()
                sample_data = await self._create_sample_training_data()
                training_data.extend(sample_data)
                print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {len(training_data)} ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ (‡∏à‡∏£‡∏¥‡∏á: {len(real_data)}, ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {len(sample_data)})")
            
            if len(training_data) > 0:
                print(f"üéØ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô GzipModel ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(training_data)} ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ...")
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô GzipModel
                self.trained_model = GzipModel(training_data)
                self.model_trained = True
                
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
                test_result = self.trained_model.predict("‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏°‡∏≤‡∏Å", k=1)
                print(f"üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•: '‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏°‡∏≤‡∏Å' -> {test_result}")
                
                print(f"‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô GzipModel ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                
            else:
                print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô ‡∏à‡∏∞‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏ä‡πâ rule-based")
                self.analysis_method = 0
                
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô GzipModel: {e}")
            import traceback
            traceback.print_exc()
            print("Will fallback to rule-based analysis")
            self.analysis_method = 0
    
    def _tokenize_thai(self, text: str) -> List[str]:
        """‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        if PYTHAINLP_AVAILABLE:
            try:
                return word_tokenize(text, keep_whitespace=False)
            except:
                pass
        
        # Basic tokenization fallback
        tokens = re.findall(r'[‡∏Å-‡∏Æ]+|[a-zA-Z]+|\d+', text)
        return tokens
    
    def _preprocess_text(self, text: str) -> str:
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        if not isinstance(text, str) or not text.strip():
            return ""
        
        try:
            text = text.strip()
            text = re.sub(r'\r\n|\r|\n', ' ', text)
            text = re.sub(r'\s+', ' ', text)
            text = re.sub(r'[^\u0e00-\u0e7fa-zA-Z0-9\s\-\.]', ' ', text)
            return text.strip()
        except Exception as e:
            print(f"Warning: preprocessing error: {e}")
            return text
    
    def _analyze_sentiment_rule_based(self, text: str) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏î‡πâ‡∏ß‡∏¢ rule-based approach (Method 0)"""
        try:
            text_lower = text.lower()
            tokens = self._tokenize_thai(text_lower)
            
            positive_count = 0
            negative_count = 0
            neutral_count = 0
            
            positive_words = []
            negative_words = []
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡πà‡∏≠‡∏ô
            if "‡πÑ‡∏°‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô" in text_lower:
                positive_count += 2
                positive_words.append("‡πÑ‡∏°‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô")
            elif "‡∏™‡∏±‡∏ö‡∏™‡∏ô" in text_lower and "‡πÑ‡∏°‡πà" not in text_lower:
                negative_count += 1
                negative_words.append("‡∏™‡∏±‡∏ö‡∏™‡∏ô")
            
            for token in tokens:
                if token in self.positive_indicators:
                    positive_count += 1
                    positive_words.append(token)
                elif token in self.negative_indicators:
                    negative_count += 1
                    negative_words.append(token)
                elif token in self.neutral_indicators:
                    neutral_count += 1
            
            total_sentiment = positive_count + negative_count + neutral_count
            
            if total_sentiment == 0:
                if any(word in text_lower for word in ['‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£', '‡∏Ñ‡∏ß‡∏£', '‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á', '‡πÄ‡∏û‡∏¥‡πà‡∏°', '‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç']):
                    sentiment = "negative"
                elif any(word in text_lower for word in ['‡πÑ‡∏°‡πà‡∏°‡∏µ', '‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà', '‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö', '-']):
                    sentiment = "neutral"
                else:
                    sentiment = "neutral"
            else:
                if positive_count > negative_count:
                    sentiment = "positive"
                elif negative_count > positive_count:
                    sentiment = "negative"
                else:
                    sentiment = "neutral"
            
            return {
                "sentiment": sentiment,
                "positive_indicators": positive_count,
                "negative_indicators": negative_count,
                "neutral_indicators": neutral_count,
                "positive_words": positive_words,
                "negative_words": negative_words,
                "method": "rule_based",
                "method_id": 0
            }
            
        except Exception as e:
            print(f"Error in rule-based sentiment analysis: {e}")
            return {
                "sentiment": "neutral",
                "positive_indicators": 0,
                "negative_indicators": 0,
                "neutral_indicators": 0,
                "positive_words": [],
                "negative_words": [],
                "method": "rule_based_error",
                "method_id": 0
            }
    
    def _analyze_sentiment_gzip_model(self, text: str) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏î‡πâ‡∏ß‡∏¢ GzipModel (Method 1)"""
        try:
            if not self.trained_model or not self.model_trained:
                # Fallback to rule-based if model not available
                print("‚ö†Ô∏è GzipModel not available, falling back to rule-based")
                return self._analyze_sentiment_rule_based(text)
            
            # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß
            predicted_sentiment = self.trained_model.predict(text, k=1)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì rule-based ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
            rule_result = self._analyze_sentiment_rule_based(text)
            
            return {
                "sentiment": predicted_sentiment,
                "positive_indicators": rule_result.get("positive_indicators", 0),
                "negative_indicators": rule_result.get("negative_indicators", 0),
                "neutral_indicators": rule_result.get("neutral_indicators", 0),
                "positive_words": rule_result.get("positive_words", []),
                "negative_words": rule_result.get("negative_words", []),
                "method": "gzip_model",
                "method_id": 1,
                "model_prediction": predicted_sentiment,
                "rule_prediction": rule_result["sentiment"],
                "agreement": predicted_sentiment == rule_result["sentiment"]
            }
            
        except Exception as e:
            print(f"Error in GzipModel analysis: {e}")
            # Fallback to rule-based
            return self._analyze_sentiment_rule_based(text)
    
    async def _analyze_sentiment_ssense(self, text: str) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏î‡πâ‡∏ß‡∏¢ SSense API (Method 2)"""
        try:
            headers = {
                'Apikey': self.ssense_api_key
            }
            params = {'text': text}
            
            async with aiohttp.ClientSession() as session:
                async with session.get(self.ssense_url, headers=headers, params=params, timeout=10) as response:
                    if response.status == 200:
                        result = await response.json()
                        
                        # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å SSense
                        sentiment_data = result.get('sentiment', {})
                        polarity = sentiment_data.get('polarity', '')
                        
                        if polarity == 'positive':
                            sentiment = "positive"
                        elif polarity == 'negative':
                            sentiment = "negative"
                        else:
                            sentiment = "neutral"
                        
                        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å preprocess
                        preprocess_data = result.get('preprocess', {})
                        positive_words = preprocess_data.get('pos', [])
                        negative_words = preprocess_data.get('neg', [])
                        keywords = preprocess_data.get('keyword', [])
                        
                        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å intention
                        intention_data = result.get('intention', {})
                        
                        return {
                            "sentiment": sentiment,
                            "positive_indicators": len(positive_words),
                            "negative_indicators": len(negative_words),
                            "neutral_indicators": 0,
                            "positive_words": positive_words,
                            "negative_words": negative_words,
                            "keywords": keywords,
                            "method": "ssense_api",
                            "method_id": 2,
                            "ssense_polarity": polarity,
                            "ssense_intention": intention_data,
                            "ssense_raw": result
                        }
                        
                    else:
                        print(f"‚ö†Ô∏è SSense API error: {response.status}")
                        # Fallback to rule-based
                        result = self._analyze_sentiment_rule_based(text)
                        result["method"] = "ssense_fallback_rule"
                        result["method_id"] = 2
                        return result
                        
        except Exception as e:
            print(f"Error in SSense API analysis: {e}")
            # Fallback to rule-based
            result = self._analyze_sentiment_rule_based(text)
            result["method"] = "ssense_fallback_error"
            result["method_id"] = 2
            return result
    
    def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ß‡πâ"""
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å method ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        method_info = self.available_methods[self.analysis_method]
        method_function = getattr(self, method_info["function"])
        
        if self.analysis_method == 2:
            # SSense API ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ async - ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤ event loop ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ fallback
            try:
                import asyncio
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ event loop ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                try:
                    loop = asyncio.get_running_loop()
                    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ loop ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ create_task ‡∏´‡∏£‡∏∑‡∏≠ fallback
                    print("‚ö†Ô∏è Event loop is running, using synchronous fallback for SSense")
                    # Fallback to rule-based
                    return self._analyze_sentiment_rule_based(text)
                except RuntimeError:
                    # ‡πÑ‡∏°‡πà‡∏°‡∏µ loop ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        return loop.run_until_complete(method_function(text))
                    finally:
                        loop.close()
                        
            except Exception as e:
                print(f"Error in SSense API: {e}, falling back to rule-based")
                return self._analyze_sentiment_rule_based(text)
        else:
            return method_function(text)
    
    def _extract_keywords_advanced(self, text: str) -> List[Dict]:
        """‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
        try:
            tokens = self._tokenize_thai(text)
            if not tokens:
                return []
            
            # ‡πÉ‡∏ä‡πâ POS tagging ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ PyThaiNLP
            if PYTHAINLP_AVAILABLE:
                try:
                    pos_tags = pos_tag(tokens)
                except:
                    pos_tags = [(token, "UNKNOWN") for token in tokens]
            else:
                pos_tags = [(token, "UNKNOWN") for token in tokens]
            
            keywords = []
            
            for word, pos in pos_tags:
                word_clean = word.strip().lower()
                
                if (len(word_clean) > 1 and 
                    not word_clean.isdigit() and
                    word_clean not in {'-', '/', '(', ')', '[', ']', '‡πÅ‡∏•‡∏∞', '‡∏ó‡∏µ‡πà', '‡∏Å‡∏≤‡∏£', '‡πÉ‡∏ô', '‡∏Ç‡∏≠‡∏á', '‡πÄ‡∏õ‡πá‡∏ô', '‡∏°‡∏µ', '‡πÑ‡∏î‡πâ', '‡∏à‡∏∞', '‡πÑ‡∏ß‡πâ', '‡∏ô‡∏µ‡πâ', '‡∏ô‡∏±‡πâ‡∏ô'}):
                    
                    base_score = 0.3
                    
                    if pos in ['ADJ', 'VERB']:
                        base_score += 0.3
                    elif pos in ['NOUN', 'ADV']:
                        base_score += 0.2
                    
                    sentiment_type = "neutral"
                    category = "general"
                    
                    if word_clean in self.positive_indicators:
                        base_score += 0.3
                        sentiment_type = "positive"
                    elif word_clean in self.negative_indicators:
                        base_score += 0.3
                        sentiment_type = "negative"
                    elif word_clean in self.neutral_indicators:
                        base_score += 0.1
                        sentiment_type = "neutral"
                    
                    for cat, keywords_list in self.survey_keywords.items():
                        if any(kw in word_clean for kw in keywords_list):
                            category = cat
                            base_score += 0.2
                            break
                    
                    keywords.append({
                        "word": word_clean,
                        "score": min(base_score, 1.0),
                        "frequency": 1,
                        "pos_tag": pos,
                        "sentiment_type": sentiment_type,
                        "category": category
                    })
            
            keywords.sort(key=lambda x: x["score"], reverse=True)
            return keywords[:8]
            
        except Exception as e:
            print(f"Error extracting keywords: {e}")
            words = text.split()
            return [{"word": word.lower(), "score": 0.5, "frequency": 1, 
                    "pos_tag": "UNKNOWN", "sentiment_type": "neutral", "category": "general"} 
                   for word in words[:3] if len(word) > 1]
    
    async def analyze_single_text(self, text: str, column_name: str = "") -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß"""
        if not text or not text.strip():
            return {
                "sentiment": "neutral",
                "keywords": [],
                "text": text,
                "column": column_name
            }
        
        try:
            processed_text = self._preprocess_text(text)
            
            if not processed_text:
                return {
                    "sentiment": "neutral",
                    "keywords": [],
                    "text": text,
                    "column": column_name
                }
            
            # ‡πÉ‡∏ä‡πâ method ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ß‡πâ - ‡πÅ‡∏¢‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ async/sync
            if self.analysis_method == 2:
                # SSense API method - ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                sentiment_result = await self._analyze_sentiment_ssense(processed_text)
            else:
                # Rule-based ‡∏´‡∏£‡∏∑‡∏≠ GzipModel
                sentiment_result = self.analyze_sentiment(processed_text)
            
            keywords = self._extract_keywords_advanced(processed_text)
            
            return {
                "sentiment": sentiment_result["sentiment"],
                "keywords": keywords,
                "text": text,
                "column": column_name,
                "debug": {
                    "processed_text": processed_text,
                    "positive_indicators": sentiment_result.get("positive_indicators", 0),
                    "negative_indicators": sentiment_result.get("negative_indicators", 0),
                    "positive_words": sentiment_result.get("positive_words", []),
                    "negative_words": sentiment_result.get("negative_words", []),
                    "method": sentiment_result.get("method", "unknown"),
                    "method_id": sentiment_result.get("method_id", self.analysis_method),
                    "analysis_method": self.analysis_method,
                    "ssense_raw": sentiment_result.get("ssense_raw") if self.analysis_method == 2 else None
                }
            }
            
        except Exception as e:
            print(f"Error analyzing text: {e}")
            return {
                "sentiment": "neutral",
                "keywords": [],
                "text": text,
                "column": column_name
            }
    
    def _identify_survey_text_columns(self, df: pd.DataFrame) -> Dict[int, str]:
        """‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ö‡∏ö‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏° Post-Test Survey ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥"""
        
        column_mapping = {}
        
        for i, col_name in enumerate(df.columns):
            col_str = str(col_name).lower()
            
            # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô demographic ‡∏´‡∏£‡∏∑‡∏≠ metadata
            if any(skip_word in col_str for skip_word in ['timestamp', '‡πÄ‡∏ß‡∏•‡∏≤', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á', 'id', '‡∏•‡∏≥‡∏î‡∏±‡∏ö', '‡∏ä‡∏∑‡πà‡∏≠', '‡∏≠‡∏≤‡∏¢‡∏∏', '‡πÄ‡∏û‡∏®', 'email']):
                continue
            
            # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Yes/No ‡∏´‡∏£‡∏∑‡∏≠ ‡∏™‡∏ô‡πÉ‡∏à/‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à
            sample_data = df.iloc[:, i].dropna().astype(str).head(20).tolist()
            unique_values = set([str(val).strip() for val in sample_data])
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡∏™‡∏ô‡πÉ‡∏à/‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à
            if (len(unique_values) <= 3 and 
                any(val in ['‡∏™‡∏ô‡πÉ‡∏à', '‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à', '‡∏¢‡∏¥‡∏ô‡∏¢‡∏≠‡∏°', '‡πÑ‡∏°‡πà‡∏¢‡∏¥‡∏ô‡∏¢‡∏≠‡∏°'] for val in unique_values)):
                print(f"üö´ ‡∏Ç‡πâ‡∏≤‡∏° column {i+1} ({col_name}): ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")
                continue
                
            if not sample_data:
                continue
                
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            text_count = 0
            
            for text in sample_data:
                text_clean = text.strip()
                
                # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô
                if (len(text_clean) <= 2 or 
                    text_clean in ['-', '‡πÑ‡∏°‡πà‡∏°‡∏µ', '‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà', '‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö', '‡∏¢‡∏¥‡∏ô‡∏¢‡∏≠‡∏°', '‡πÑ‡∏°‡πà‡∏¢‡∏¥‡∏ô‡∏¢‡∏≠‡∏°', '‡∏™‡∏ô‡πÉ‡∏à', '‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à'] or
                    text_clean.isdigit() or
                    text_clean in ['1', '2', '3', '4', '5'] or
                    text_clean in ['1.0', '2.0', '3.0', '4.0', '5.0']):
                    continue
                    
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (choice) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                if any(choice in text_clean for choice in [
                    '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏£‡∏≤‡∏™‡∏≤‡∏£‡∏´‡∏ô‡∏µ‡πâ', '‡∏Ç‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£', '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡πÄ‡∏á‡∏¥‡∏ô',
                    'Email', 'OTP', 'ThaiD', '‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô'
                ]):
                    continue
                    
                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô
                if (any('\u0e00' <= char <= '\u0e7f' for char in text_clean) and 
                    len(text_clean) > 3):
                    text_count += 1
                    
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 30% ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            if text_count >= max(1, len(sample_data) * 0.3):
                # ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡∏°‡πà
                if '‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•' in col_str and '‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö' in col_str:
                    column_mapping[i] = "‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö"
                elif '‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ' in col_str or '‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î' in col_str:
                    column_mapping[i] = "‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"
                elif '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°' in col_str or '‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°' in col_str:
                    column_mapping[i] = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"
                elif '‡∏™‡∏±‡∏ö‡∏™‡∏ô' in col_str and '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà' in col_str and '‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•' in col_str:
                    column_mapping[i] = "‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"
                elif '‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á' in col_str or '‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç' in col_str:
                    column_mapping[i] = "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á"
                elif '‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥' in col_str or '‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°' in col_str:
                    column_mapping[i] = "‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"
                else:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÜ
                    print(f"ü§î ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö column {i+1} ({col_name}): {text_count}/{len(sample_data)} texts")
                    if text_count >= 3:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
                        column_mapping[i] = f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô_{str(col_name)}"
        
        print(f"üîç ‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {len(column_mapping)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
        for col_idx, col_name in column_mapping.items():
            print(f"  - ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå {col_idx + 1}: {col_name}")
            
        return column_mapping
    
    def _analyze_likert_scales(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Likert Scale ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"""
        likert_analysis = {}
        
        for i, col_name in enumerate(df.columns):
            col_str = str(col_name).lower()
            
            # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà likert scale
            if any(skip_word in col_str for skip_word in ['timestamp', '‡πÄ‡∏ß‡∏•‡∏≤', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'id', '‡∏•‡∏≥‡∏î‡∏±‡∏ö', 'email', '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡∏ä‡∏∑‡πà‡∏≠']):
                continue
                
            try:
                col_data = df.iloc[:, i]
                numeric_data = pd.to_numeric(col_data, errors='coerce').dropna()
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô likert scale (‡∏Ñ‡πà‡∏≤ 1-5) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                unique_values = set(numeric_data)
                is_likert = (len(unique_values) <= 5 and 
                           all(val in range(1, 6) for val in unique_values) and
                           len(numeric_data) > 0)
                
                if is_likert:
                    valid_scores = numeric_data[(numeric_data >= 1) & (numeric_data <= 5)]
                    
                    if len(valid_scores) > 0:
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ
                        if '‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô' in col_str or 'register' in col_str:
                            description = "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"
                        elif '‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°' in col_str or 'overall' in col_str:
                            description = "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°"
                        elif '‡πÄ‡∏ß‡∏•‡∏≤' in col_str or 'time' in col_str:
                            description = "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤"
                        elif '‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à' in col_str or 'confidence' in col_str:
                            description = "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à"
                        elif '‡∏û‡∏≠‡πÉ‡∏à' in col_str or 'satisfaction' in col_str:
                            description = "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏≠‡πÉ‡∏à"
                        else:
                            description = f"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô_{str(col_name)}"
                            
                        likert_analysis[description] = {
                            "mean": float(valid_scores.mean()),
                            "std": float(valid_scores.std()) if len(valid_scores) > 1 else 0.0,
                            "distribution": valid_scores.value_counts().sort_index().to_dict(),
                            "count": len(valid_scores),
                            "original_column": i,
                            "column_name": str(col_name)
                        }
                        
            except Exception as e:
                print(f"Could not analyze column {i} ({col_name}): {e}")
                continue
        
        return likert_analysis
    
    def _analyze_choice_questions(self, df: pd.DataFrame) -> Dict[str, Dict[str, int]]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≠‡∏ö"""
        choice_analysis = {}
        
        try:
            for i, col_name in enumerate(df.columns):
                col_str = str(col_name).lower()
                
                # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
                if any(skip_word in col_str for skip_word in ['timestamp', '‡πÄ‡∏ß‡∏•‡∏≤', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'id', '‡∏•‡∏≥‡∏î‡∏±‡∏ö', '‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•']):
                    continue
                
                # ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
                col_data = df.iloc[:, i].dropna()
                if len(col_data) == 0:
                    continue
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                value_counts = col_data.value_counts()
                unique_values = set(col_data.astype(str))
                
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡πÜ ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà likert scale)
                is_choice = (len(unique_values) <= 10 and 
                           len(value_counts) > 1 and
                           not all(str(val).isdigit() and 1 <= int(val) <= 5 for val in unique_values if str(val).isdigit()))
                
                if is_choice:
                    if '‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î' in col_str or 'best' in col_str:
                        choice_analysis["‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"] = value_counts.to_dict()
                    elif '‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö' in col_str or 'login' in col_str:
                        choice_analysis["‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö"] = value_counts.to_dict()
                    elif '‡∏™‡∏±‡∏ö‡∏™‡∏ô' in col_str and '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà' in col_str and '‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•' not in col_str:
                        choice_analysis["‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏ö‡∏™‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"] = value_counts.to_dict()
                    elif ('‡∏ó‡∏î‡∏™‡∏≠‡∏ö' in col_str and '‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á' in col_str) or ('‡∏™‡∏ô‡πÉ‡∏à' in col_str):
                        choice_analysis["‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"] = value_counts.to_dict()
                    else:
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô choice questions ‡∏à‡∏£‡∏¥‡∏á‡πÜ
                        choice_keywords = ['‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡∏Ç‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£', 'Email', 'OTP', 'ThaiD']
                        if any(any(keyword in str(val) for keyword in choice_keywords) for val in unique_values):
                            choice_analysis[f"‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≠‡∏ö_{str(col_name)}"] = value_counts.to_dict()
                        
        except Exception as e:
            print(f"Error analyzing choice questions: {e}")
        
        return choice_analysis
    
    async def generate_ai_insights(self, sentiment_dist: Dict, column_analysis: Dict, 
                                 keywords: List, detailed_results: List, 
                                 likert_analysis: Dict, choice_analysis: Dict) -> Dict:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á AI Insights ‡∏î‡πâ‡∏ß‡∏¢ Gemini API"""
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ AI
        summary_data = {
            "sentiment_summary": sentiment_dist,
            "top_keywords": [kw["word"] for kw in keywords[:10]],
            "negative_feedback_samples": [
                result["text"] for result in detailed_results 
                if result["sentiment"] == "negative"
            ][:5],
            "positive_feedback_samples": [
                result["text"] for result in detailed_results 
                if result["sentiment"] == "positive"
            ][:5],
            "likert_scores": {k: v.get("mean", 0) for k, v in likert_analysis.items()},
            "choice_results": choice_analysis
        }
        
        # ‡πÉ‡∏ä‡πâ Gemini API ‡∏´‡∏£‡∏∑‡∏≠ fallback ‡πÄ‡∏õ‡πá‡∏ô rule-based
        try:
            # TODO: ‡πÄ‡∏û‡∏¥‡πà‡∏° Gemini API call ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
            ai_insights = await self._call_gemini_api(summary_data)
            return ai_insights
        except Exception as e:
            print(f"AI API failed, using fallback: {e}")
            return self._generate_survey_insights_fallback(
                sentiment_dist, column_analysis, keywords, detailed_results, 
                likert_analysis, choice_analysis
            )
    
    async def _call_gemini_api(self, data: Dict) -> Dict:
        """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Gemini API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á insights"""
        # TODO: implement Gemini API call
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ fallback
        raise Exception("Gemini API not implemented yet")
    
    def _generate_survey_insights_fallback(self, sentiment_dist: Dict, column_analysis: Dict, 
                                 keywords: List, detailed_results: List, 
                                 likert_analysis: Dict, choice_analysis: Dict) -> Dict:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á insights ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Post-Test Survey (Fallback)"""
        insights = {
            "positive_aspects": [],
            "negative_aspects": [],
            "recommendations": [],
            "system_strengths": [],
            "improvement_areas": [],
            "user_pain_points": []
        }
        
        total_analyzed = sum(sentiment_dist.values())
        if total_analyzed == 0:
            return insights
        
        positive_percent = (sentiment_dist["positive"] / total_analyzed) * 100
        negative_percent = (sentiment_dist["negative"] / total_analyzed) * 100
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å sentiment
        if positive_percent > 60:
            insights["positive_aspects"].append(f"‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à‡∏™‡∏π‡∏á ({positive_percent:.1f}%)")
        elif positive_percent < 40:
            insights["negative_aspects"].append(f"‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à‡∏ï‡πà‡∏≥ ({positive_percent:.1f}%)")
            
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å keywords
        positive_keywords = [kw["word"] for kw in keywords[:10] 
                           if kw.get("sentiment_type") == "positive"]
        negative_keywords = [kw["word"] for kw in keywords[:10] 
                           if kw.get("sentiment_type") == "negative"]
        
        if positive_keywords:
            insights["system_strengths"].append(f"‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á: {', '.join(positive_keywords[:5])}")
            
        if negative_keywords:
            insights["user_pain_points"].append(f"‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å: {', '.join(negative_keywords[:5])}")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å likert analysis
        if likert_analysis:
            high_scores = {k: v for k, v in likert_analysis.items() if v.get("mean", 0) >= 4.0}
            low_scores = {k: v for k, v in likert_analysis.items() if v.get("mean", 0) < 3.5}
            
            if high_scores:
                best_aspect = max(high_scores.items(), key=lambda x: x[1].get("mean", 0))
                insights["positive_aspects"].append(f"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {best_aspect[0]} ({best_aspect[1].get('mean', 0):.2f}/5)")
            
            if low_scores:
                worst_aspect = min(low_scores.items(), key=lambda x: x[1].get("mean", 0))
                insights["improvement_areas"].append(f"‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô: {worst_aspect[0]} ({worst_aspect[1].get('mean', 0):.2f}/5)")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å choice questions - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ô‡πÉ‡∏à/‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à
        if choice_analysis:
            for question, answers in choice_analysis.items():
                if "‡∏™‡∏ô‡πÉ‡∏à" in question:
                    total_respondents = sum(answers.values())
                    interested = answers.get("‡∏™‡∏ô‡πÉ‡∏à", 0)
                    if total_respondents > 0:
                        interest_rate = (interested / total_respondents) * 100
                        if interest_rate >= 70:
                            insights["positive_aspects"].append(f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏π‡∏á ({interest_rate:.1f}%)")
                        elif interest_rate < 50:
                            insights["negative_aspects"].append(f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≥ ({interest_rate:.1f}%)")
                elif "‡∏™‡∏±‡∏ö‡∏™‡∏ô" in question:
                    confused_count = sum(count for answer, count in answers.items() if "‡∏™‡∏±‡∏ö‡∏™‡∏ô" in str(answer))
                    total_count = sum(answers.values())
                    if confused_count > total_count * 0.3:
                        insights["negative_aspects"].append(f"‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏±‡∏ö‡∏™‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ({confused_count}/{total_count})")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å detailed results
        common_issues = []
        positive_themes = []
        
        for result in detailed_results:
            text_lower = result["text"].lower()
            
            # ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢
            if "‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢" in text_lower and ("‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠" in text_lower or "‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à" in text_lower):
                common_issues.append("‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢")
            if "‡∏õ‡∏∏‡πà‡∏°" in text_lower and ("edit" in text_lower or "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç" in text_lower or "‡∏´‡∏≤" in text_lower):
                common_issues.append("‡∏õ‡∏∏‡πà‡∏°‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏≤‡∏¢‡∏≤‡∏Å")
            if "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà" in text_lower and "‡∏™‡∏±‡∏ö‡∏™‡∏ô" in text_lower:
                common_issues.append("‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô")
            
            # ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢
            if result["sentiment"] == "positive":
                if "‡∏™‡∏∞‡∏î‡∏ß‡∏Å" in text_lower or "‡∏á‡πà‡∏≤‡∏¢" in text_lower:
                    positive_themes.append("‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡πÅ‡∏•‡∏∞‡∏á‡πà‡∏≤‡∏¢")
                if "‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£" in text_lower:
                    positive_themes.append("‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£")
                if "‡πÄ‡∏£‡πá‡∏ß" in text_lower or "‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß" in text_lower:
                    positive_themes.append("‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß")
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° insights
        unique_issues = list(set(common_issues))
        unique_themes = list(set(positive_themes))
        
        if unique_issues:
            insights["negative_aspects"].extend(unique_issues[:3])
        if unique_themes:
            insights["system_strengths"].extend(unique_themes[:3])
            
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞
        recommendations = []
        
        if "‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢" in unique_issues:
            recommendations.append("‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô")
        if "‡∏õ‡∏∏‡πà‡∏°‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏≤‡∏¢‡∏≤‡∏Å" in unique_issues:
            recommendations.append("‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏∏‡πà‡∏°‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô")
        if "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô" in unique_issues:
            recommendations.append("‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô ‡∏ß‡∏±‡∏ô-‡πÄ‡∏î‡∏∑‡∏≠‡∏ô-‡∏õ‡∏µ (‡∏û.‡∏®.)")
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        if negative_percent > positive_percent:
            recommendations.append("‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á UX/UI ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à")
        if likert_analysis and any(v.get("mean", 0) < 3.5 for v in likert_analysis.values()):
            recommendations.append("‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à")
        
        recommendations.extend([
            "‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô",
            "‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö help ‡πÅ‡∏•‡∏∞ FAQ"
        ])
        
        insights["recommendations"] = recommendations[:5]
        
        return insights
    
    async def analyze_survey(self, df: pd.DataFrame, analysis_id: str) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ö‡∏ö‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏° Post-Test Survey ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô"""
        print(f"üéØ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Post-Test Survey ID: {analysis_id}")
        method_name = self.available_methods[self.analysis_method]['name']
        print(f"üìä ‡πÉ‡∏ä‡πâ {method_name} (Method {self.analysis_method})")
        print(f"üìè ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {df.shape[0]} ‡πÅ‡∏ñ‡∏ß, {df.shape[1]} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
        
        results = {
            "total_responses": len(df),
            "texts_analyzed": 0,
            "sentiment_distribution": {"positive": 0, "neutral": 0, "negative": 0},
            "top_keywords": [],
            "detailed_results": [],
            "column_analysis": {},
            "model_info": {
                "engine": method_name,
                "preprocessing": "Thai tokenization + Advanced sentiment rules",
                "features": "Survey-specific keyword analysis + POS enhancement",
                "version": "Post-Test Survey Optimized v3.0 + Multiple Methods",
                "analysis_method": self.analysis_method,
                "method_name": method_name,
                "available_methods": self.get_available_methods()
            },
            "insights": {}
        }
        
        # ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        text_columns = self._identify_survey_text_columns(df)
        print(f"üìù ‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {len(text_columns)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Likert scales
        likert_analysis = self._analyze_likert_scales(df)
        results["likert_analysis"] = likert_analysis
        print(f"üìè ‡∏û‡∏ö Likert scales: {len(likert_analysis)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Choice questions
        choice_analysis = self._analyze_choice_questions(df)
        results["choice_analysis"] = choice_analysis
        print(f"‚òëÔ∏è ‡∏û‡∏ö Choice questions: {len(choice_analysis)} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        all_keywords = {}
        
        for col_idx, col_description in text_columns.items():
            print(f"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: {col_description}")
            
            column_results = {
                "total_texts": 0,
                "sentiment_dist": {"positive": 0, "neutral": 0, "negative": 0},
                "sample_texts": {"positive": [], "negative": [], "neutral": []},
                "keywords_by_category": {}
            }
            
            if col_idx < len(df.columns):
                texts = df.iloc[:, col_idx].dropna().astype(str).tolist()
                
                for text in texts:
                    text_clean = text.strip()
                    
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
                    if (not text_clean or 
                        len(text_clean) <= 2 or 
                        text_clean in ['-', '‡πÑ‡∏°‡πà‡∏°‡∏µ', '‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà', '‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö', '‡∏¢‡∏¥‡∏ô‡∏¢‡∏≠‡∏°', '‡πÑ‡∏°‡πà‡∏¢‡∏¥‡∏ô‡∏¢‡∏≠‡∏°', '‡∏™‡∏ô‡πÉ‡∏à', '‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à'] or
                        text_clean.isdigit()):
                        continue
                        
                    analysis = await self.analyze_single_text(text_clean, col_description)
                    
                    results["detailed_results"].append({
                        "column": col_description,
                        "text": text_clean,
                        **analysis
                    })
                    
                    sentiment = analysis["sentiment"]
                    if sentiment in results["sentiment_distribution"]:
                        results["sentiment_distribution"][sentiment] += 1
                        column_results["sentiment_dist"][sentiment] += 1
                    
                    column_results["total_texts"] += 1
                    
                    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                    if len(column_results["sample_texts"][sentiment]) < 3:
                        column_results["sample_texts"][sentiment].append(text_clean[:150])
                    
                    # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° keywords
                    for kw in analysis["keywords"]:
                        word = kw["word"]
                        category = kw.get("category", "general")
                        
                        if word not in all_keywords:
                            all_keywords[word] = {
                                "count": 0, 
                                "total_score": 0, 
                                "sentiment_type": kw.get("sentiment_type", "neutral"),
                                "category": category,
                                "columns": set()
                            }
                        
                        all_keywords[word]["count"] += 1
                        all_keywords[word]["total_score"] += kw["score"]
                        all_keywords[word]["columns"].add(col_description)
                        
                        if category not in column_results["keywords_by_category"]:
                            column_results["keywords_by_category"][category] = {}
                        if word not in column_results["keywords_by_category"][category]:
                            column_results["keywords_by_category"][category][word] = 0
                        column_results["keywords_by_category"][category][word] += 1
            
            if column_results["total_texts"] > 0:
                results["column_analysis"][col_description] = column_results
                print(f"  ‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡πâ‡∏ß: {column_results['total_texts']} ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°")
        
        results["texts_analyzed"] = len(results["detailed_results"])
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ keywords
        sorted_keywords = sorted(
            all_keywords.items(),
            key=lambda x: (x[1]["count"], x[1]["total_score"]),
            reverse=True
        )[:25]
        
        results["top_keywords"] = [
            {
                "word": word,
                "count": data["count"],
                "avg_score": data["total_score"] / data["count"],
                "sentiment_type": data["sentiment_type"],
                "category": data["category"],
                "appears_in_columns": len(data["columns"])
            }
            for word, data in sorted_keywords
        ]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á AI insights
        results["insights"] = await self.generate_ai_insights(
            results["sentiment_distribution"], 
            results["column_analysis"],
            results["top_keywords"],
            results["detailed_results"],
            likert_analysis,
            choice_analysis
        )
        
        print(f"‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô:")
        print(f"  üìä ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {results['texts_analyzed']}")
        print(f"  üìà Sentiment: +{results['sentiment_distribution']['positive']} ={results['sentiment_distribution']['neutral']} -{results['sentiment_distribution']['negative']}")
        print(f"  üîë Keywords: {len(results['top_keywords'])} ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç")
        print(f"  üìè Likert scales: {len(likert_analysis)} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
        print(f"  ‚òëÔ∏è Choice questions: {len(choice_analysis)} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
        print(f"  ü§ñ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {method_name} (Method {self.analysis_method})")
        
        return results
    
    def is_ready(self) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"""
        return self._ready
    
    def get_model_info(self) -> Dict[str, Any]:
        """‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"""
        method_info = self.available_methods[self.analysis_method]
        
        return {
            "analysis_method": self.analysis_method,
            "method_name": method_info["name"],
            "pythainlp_available": PYTHAINLP_AVAILABLE,
            "training_data_path": self.training_data_path,
            "models_path": self.models_path,
            "available_methods": self.get_available_methods(),
            "features": {   
                "thai_tokenization": PYTHAINLP_AVAILABLE,
                "pos_tagging": PYTHAINLP_AVAILABLE,
                "gzip_model": self.analysis_method == 1 and self.model_trained,
                "ssense_api": self.analysis_method == 2,
                "rule_based": True,
                "survey_specific_keywords": True,
                "advanced_preprocessing": True,
                "method_switching": True
            },
            "method_specific": {
                "gzip_trained": self.model_trained if self.analysis_method == 1 else False,
                "training_data_available": os.path.exists(os.path.join(self.training_data_path, "pos.txt")),
                "ssense_api_key": bool(self.ssense_api_key) if self.analysis_method == 2 else False
            }
        }
    
    async def retrain_model(self, new_training_data: List[Tuple[str, str]] = None) -> bool:
        """‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GzipModel ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)"""
        try:
            if self.analysis_method != 1:
                print(f"‚ùå ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ GzipModel (Method 1) ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")
                print(f"   ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÉ‡∏ä‡πâ {self.available_methods[self.analysis_method]['name']} (Method {self.analysis_method})")
                return False
                
            if not PYTHAINLP_AVAILABLE:
                print("‚ùå PyThaiNLP ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ")
                return False
            
            print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏£‡∏ô GzipModel ‡πÉ‡∏´‡∏°‡πà...")
            
            # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå
            if new_training_data:
                training_data = new_training_data
                print(f"üìù ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà: {len(training_data)} ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ")
            else:
                # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
                await self._load_and_train_gzip_model()
                return self.model_trained
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà
            self.trained_model = GzipModel(training_data)
            self.model_trained = True
            
            print(f"‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô GzipModel ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
            return True
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà: {e}")
            return False
    
    async def predict_sentiment_async(self, text: str) -> Dict[str, Any]:
        """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ sentiment ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß (async version)"""
        try:
            processed_text = self._preprocess_text(text)
            
            if self.analysis_method == 2:
                # SSense API method
                result = await self._analyze_sentiment_ssense(processed_text)
            else:
                # Rule-based ‡∏´‡∏£‡∏∑‡∏≠ GzipModel
                result = self.analyze_sentiment(processed_text)
            
            return {
                "text": text,
                "processed_text": processed_text,
                "predicted_sentiment": result["sentiment"],
                "method": result.get("method", "unknown"),
                "method_id": result.get("method_id", self.analysis_method),
                "analysis_method": self.analysis_method,
                "method_name": self.available_methods[self.analysis_method]["name"],
                "ssense_data": result.get("ssense_raw") if self.analysis_method == 2 else None
            }
            
        except Exception as e:
            return {
                "text": text,
                "error": str(e),
                "predicted_sentiment": "neutral",
                "analysis_method": self.analysis_method
            }
    
    def predict_sentiment(self, text: str) -> Dict[str, Any]:
        """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ sentiment ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö)"""
        try:
            processed_text = self._preprocess_text(text)
            
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SSense API ‡πÉ‡∏´‡πâ fallback ‡πÄ‡∏õ‡πá‡∏ô rule-based ‡πÉ‡∏ô sync context
            if self.analysis_method == 2:
                print("‚ö†Ô∏è SSense API not available in sync context, using rule-based fallback")
                original_method = self.analysis_method
                self.analysis_method = 0
                result = self.analyze_sentiment(processed_text)
                self.analysis_method = original_method
                result["method"] = "ssense_sync_fallback"
            else:
                result = self.analyze_sentiment(processed_text)
            
            return {
                "text": text,
                "processed_text": processed_text,
                "predicted_sentiment": result["sentiment"],
                "method": result.get("method", "unknown"),
                "method_id": result.get("method_id", self.analysis_method),
                "analysis_method": self.analysis_method,
                "method_name": self.available_methods[self.analysis_method]["name"]
            }
            
        except Exception as e:
            return {
                "text": text,
                "error": str(e),
                "predicted_sentiment": "neutral",
                "analysis_method": self.analysis_method
            }
    
    async def _create_sample_training_data(self) -> List[Tuple[str, str]]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
        sample_data = [
            # Positive samples
            ("‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏°‡∏≤‡∏Å", "positive"),
            ("‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏™‡∏ö‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£", "positive"),
            ("‡πÄ‡∏£‡πá‡∏ß‡∏î‡∏µ ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤", "positive"),
            ("‡∏ä‡∏≠‡∏ö‡∏°‡∏≤‡∏Å ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏•‡∏¢", "positive"),
            ("‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏°‡∏≤‡∏Å", "positive"),
            
            # Negative samples  
            ("‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å ‡∏™‡∏±‡∏ö‡∏™‡∏ô‡∏°‡∏≤‡∏Å", "negative"),
            ("‡∏¢‡∏∏‡πà‡∏á‡∏¢‡∏≤‡∏Å ‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à", "negative"),
            ("‡∏õ‡∏∏‡πà‡∏°‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏≤‡∏¢‡∏≤‡∏Å", "negative"),
            ("‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢", "negative"),
            ("‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö", "negative"),
            
            # Neutral samples
            ("‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£", "neutral"),
            ("‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ ‡∏û‡∏≠‡πÉ‡∏ä‡πâ", "neutral"),
            ("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô", "neutral"),
            ("‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö", "neutral"),
            ("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "neutral")
        ]
        
        print(f"üìù ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {len(sample_data)} ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ")
        return sample_data


# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
async def main():
    """‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Enhanced NLP Processor with SSense API"""
    
    print("üöÄ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö NLP Processor with Multiple Methods\n")
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á 3 methods
    for method in [0, 1, 2]:
        print(f"\n{'='*60}")
        print(f"üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Method {method}: {['Rule-based', 'GzipModel', 'SSense API'][method]}")
        print(f"{'='*60}")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á processor
        processor = NLPProcessor(analysis_method=method)
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏∞‡∏ö‡∏ö
        await processor.initialize()
        
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•
        model_info = processor.get_model_info()
        print(f"\nüìã ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•:")
        print(f"   Method: {model_info['method_name']} (ID: {model_info['analysis_method']})")
        print(f"   PyThaiNLP: {'‚úÖ' if model_info['pythainlp_available'] else '‚ùå'}")
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö sentiment analysis
        test_texts = [
            "‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏°‡∏≤‡∏Å ‡∏ä‡∏≠‡∏ö‡πÄ‡∏•‡∏¢",
            "‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å ‡∏™‡∏±‡∏ö‡∏™‡∏ô‡∏°‡∏≤‡∏Å", 
            "‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£",
            "‡∏õ‡∏∏‡πà‡∏°‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏≤‡∏¢‡∏≤‡∏Å ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á",
            "‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏™‡∏ö‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£"
        ]
        
        print(f"\nüß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment:")
        for text in test_texts:
            result = processor.predict_sentiment(text)
            print(f"  üìù '{text}'")
            print(f"     ‚Üí {result['predicted_sentiment']}")
            print(f"     ‚Üí ‡∏ß‡∏¥‡∏ò‡∏µ: {result['method']}")
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô method
        available_methods = processor.get_available_methods()
        print(f"\nüîÑ Methods ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ: {list(available_methods.keys())}")
        for mid, minfo in available_methods.items():
            print(f"   {mid}: {minfo['name']}")
    
    print(f"\n{'='*60}")
    print("‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
    print(f"{'='*60}")

if __name__ == "__main__":
    asyncio.run(main())