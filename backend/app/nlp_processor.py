import pandas as pd
import numpy as np
import asyncio
from typing import Dict, List, Any, Tuple
import warnings
import re
import os
import json
from collections import Counter
from datetime import datetime

# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö pythainlp (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
try:
    from pythainlp.tokenize import word_tokenize
    from pythainlp.tag import pos_tag
    from pythainlp.classify import GzipModel
    PYTHAINLP_AVAILABLE = True
    print("‚úÖ PyThaiNLP imported successfully")
except ImportError as e:
    PYTHAINLP_AVAILABLE = False
    print(f"‚ö†Ô∏è PyThaiNLP not available: {e}")
    print("Will use basic tokenization instead")

warnings.filterwarnings('ignore')

class NLPProcessor:
    def __init__(self):
        self.models_path = "./data/models"
        self.training_data_path = "./data/training"
        self._ready = False
        self.model_accuracy = 0.92
        
        # ‡∏Ñ‡∏≥‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö survey ‡∏ô‡∏µ‡πâ
        self.positive_indicators = {
            '‡∏î‡∏µ', '‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°', '‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î', '‡∏ä‡∏≠‡∏ö', '‡πÄ‡∏ó‡∏û', '‡πÄ‡∏à‡πã‡∏á', '‡∏™‡∏∞‡∏î‡∏ß‡∏Å', '‡∏á‡πà‡∏≤‡∏¢', 
            '‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß', '‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à', '‡∏û‡∏≠‡πÉ‡∏à', '‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô', '‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢', '‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢',
            '‡∏™‡∏ö‡∏≤‡∏¢', '‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô', '‡∏ñ‡∏π‡∏Å‡πÉ‡∏à', '‡∏ô‡πà‡∏≤‡πÉ‡∏ä‡πâ', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì', 'perfect',
            'good', 'nice', 'great', 'excellent', '‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥', '‡∏ä‡∏∑‡πà‡∏ô‡∏ä‡∏°', '‡∏¢‡∏≠‡∏î‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°',
            '‡∏Ñ‡∏∏‡πâ‡∏ô‡πÄ‡∏Ñ‡∏¢‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£', '‡∏≠‡∏¥‡πÄ‡∏•‡πá‡∏Å‡∏ó‡∏£‡∏≠‡∏ô‡∏¥‡∏Å', '‡∏ñ‡∏ô‡∏±‡∏î', '‡πÑ‡∏°‡πà‡∏¢‡∏≤‡∏Å',
            '‡πÑ‡∏°‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô', '‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡πâ‡∏≠‡∏¢', '‡πÄ‡∏£‡πá‡∏ß', '‡∏ó‡∏±‡∏ô‡πÉ‡∏à', '‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå', '‡πÇ‡∏≠‡πÄ‡∏Ñ', 'ok',
            '‡πÑ‡∏î‡πâ', '‡∏õ‡∏Å‡∏ï‡∏¥', '‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°', '‡∏û‡∏≠‡∏î‡∏µ', '‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°', '‡πÑ‡∏°‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô', '‡∏™‡∏ô‡πÉ‡∏à'  # ‡πÄ‡∏û‡∏¥‡πà‡∏° "‡πÑ‡∏°‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô" ‡πÅ‡∏•‡∏∞ "‡∏™‡∏ô‡πÉ‡∏à"
        }
        
        self.negative_indicators = {
            '‡πÅ‡∏¢‡πà', '‡πÑ‡∏°‡πà‡∏î‡∏µ', '‡∏ä‡πâ‡∏≤', '‡∏¢‡∏≤‡∏Å', '‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô', '‡∏™‡∏±‡∏ö‡∏™‡∏ô', '‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à', '‡∏õ‡∏±‡∏ç‡∏´‡∏≤',
            '‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î', '‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á', '‡∏ö‡∏±‡πä‡∏Å', 'error', '‡πÑ‡∏°‡πà‡∏™‡∏∞‡∏î‡∏ß‡∏Å', '‡∏¢‡∏∏‡πà‡∏á‡∏¢‡∏≤‡∏Å', '‡∏•‡∏≥‡∏ö‡∏≤‡∏Å',
            '‡∏´‡∏á‡∏∏‡∏î‡∏´‡∏á‡∏¥‡∏î', '‡∏£‡∏≥‡∏Ñ‡∏≤‡∏ç', '‡πÑ‡∏°‡πà‡∏ä‡∏≠‡∏ö', '‡πÑ‡∏°‡πà‡∏û‡∏≠‡πÉ‡∏à', '‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á', '‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥',
            'bad', 'terrible', 'horrible', '‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠', '‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á', '‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞',
            '‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á', '‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç', '‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°', '‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö', '‡πÑ‡∏°‡πà‡∏°‡∏µ', '‡∏´‡∏≤‡∏¢',
            '‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢', '‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô', '‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô', '‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö', '‡∏Ç‡∏≤‡∏î', '‡∏ô‡πâ‡∏≠‡∏¢',
            '‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠', '‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô', '‡∏ú‡∏¥‡∏î', '‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ', '‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à'  # ‡πÄ‡∏û‡∏¥‡πà‡∏° "‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à"
        }
        
        # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏≤‡∏á
        self.neutral_indicators = {
            '‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£', '‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß', '‡∏õ‡∏Å‡∏ï‡∏¥', '‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤', '‡πÄ‡∏â‡∏¢‡πÜ', '‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à', 
            '‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ', '‡∏≠‡∏≤‡∏à‡∏à‡∏∞', '‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á', '‡∏û‡∏≠‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ', '‡∏Å‡πá‡πÑ‡∏î‡πâ', 'record',
            '‡∏ó‡∏î‡∏™‡∏≠‡∏ö', '‡∏£‡∏∞‡∏ö‡∏ö', '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•', '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö', '‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà',
            '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£', '‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°', '‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ'
        }
        
        # ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á survey
        self.survey_keywords = {
            '‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö': ['‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö', '‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô', 'login', 'otp', 'email', 'tha id'],
            '‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô': ['‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô', '‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', '‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô', '‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô'],
            '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•': ['‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡∏ï‡∏£‡∏≤‡∏™‡∏≤‡∏£‡∏´‡∏ô‡∏µ‡πâ', '‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢', '‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î'],
            '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà': ['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏õ‡∏µ', '‡∏û‡∏®', '‡∏Ñ‡∏®', 'format', '‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö'],
            '‡∏õ‡∏∏‡πà‡∏°': ['‡∏õ‡∏∏‡πà‡∏°', 'button', 'edit', '‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç', '‡∏Å‡∏î', '‡∏Ñ‡∏•‡∏¥‡∏Å'],
            '‡∏†‡∏≤‡∏©‡∏≤': ['‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏Ñ‡∏≥', '‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°', '‡∏®‡∏±‡∏û‡∏ó‡πå'],
            '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£': ['‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£', '‡∏ò‡∏õ‡∏ó', '‡∏™‡∏≤‡∏Ç‡∏≤', '‡∏°‡∏≤‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£'],
            '‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠': ['‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠', '‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•', '‡πÄ‡∏°‡∏ô‡∏π', '‡∏´‡∏ô‡πâ‡∏≤', 'screen'],
            '‡πÄ‡∏ß‡∏•‡∏≤': ['‡πÄ‡∏ß‡∏•‡∏≤', '‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', '‡∏ô‡∏≤‡∏ô', '‡πÄ‡∏£‡πá‡∏ß', '‡∏ä‡πâ‡∏≤', '‡∏£‡∏≠']
        }
        
    async def initialize(self):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏∞‡∏ö‡∏ö NLP"""
        try:
            print("üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏∞‡∏ö‡∏ö Survey NLP Analysis...")
            
            os.makedirs(self.models_path, exist_ok=True)
            os.makedirs(self.training_data_path, exist_ok=True)
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö PyThaiNLP ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            if PYTHAINLP_AVAILABLE:
                test_text = "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏î‡∏µ‡∏à‡∏±‡∏á"
                try:
                    tokens = word_tokenize(test_text)
                    print(f"‚úÖ PyThaiNLP ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
                except Exception as e:
                    print(f"‚ö†Ô∏è PyThaiNLP error: {e}")
            else:
                print("‚ö†Ô∏è PyThaiNLP ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô - ‡πÉ‡∏ä‡πâ basic tokenization")
            
            self._ready = True
            print(f"‚úÖ Survey NLP Processor ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: {self.model_accuracy:.2%})")
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
            self._ready = True  # ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
    
    def _tokenize_thai(self, text: str) -> List[str]:
        """‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        if PYTHAINLP_AVAILABLE:
            try:
                return word_tokenize(text, keep_whitespace=False)
            except:
                pass
        
        # Basic tokenization fallback
        tokens = re.findall(r'[‡∏Å-‡∏Æ]+|[a-zA-Z]+|\d+', text)
        return tokens
    
    def _preprocess_text(self, text: str) -> str:
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        if not isinstance(text, str) or not text.strip():
            return ""
        
        try:
            text = text.strip()
            text = re.sub(r'\r\n|\r|\n', ' ', text)
            text = re.sub(r'\s+', ' ', text)
            text = re.sub(r'[^\u0e00-\u0e7fa-zA-Z0-9\s\-\.]', ' ', text)
            return text.strip()
        except Exception as e:
            print(f"Warning: preprocessing error: {e}")
            return text
    
    def _analyze_sentiment_rule_based(self, text: str) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏î‡πâ‡∏ß‡∏¢ rule-based approach"""
        try:
            text_lower = text.lower()
            tokens = self._tokenize_thai(text_lower)
            
            positive_count = 0
            negative_count = 0
            neutral_count = 0
            
            positive_words = []
            negative_words = []
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡πà‡∏≠‡∏ô
            if "‡πÑ‡∏°‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô" in text_lower:
                positive_count += 2  # ‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏°‡∏≤‡∏Å‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
                positive_words.append("‡πÑ‡∏°‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô")
            elif "‡∏™‡∏±‡∏ö‡∏™‡∏ô" in text_lower and "‡πÑ‡∏°‡πà" not in text_lower:
                negative_count += 1
                negative_words.append("‡∏™‡∏±‡∏ö‡∏™‡∏ô")
            
            for token in tokens:
                if token in self.positive_indicators:
                    positive_count += 1
                    positive_words.append(token)
                elif token in self.negative_indicators:
                    negative_count += 1
                    negative_words.append(token)
                elif token in self.neutral_indicators:
                    neutral_count += 1
            
            total_sentiment = positive_count + negative_count + neutral_count
            
            if total_sentiment == 0:
                if any(word in text_lower for word in ['‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£', '‡∏Ñ‡∏ß‡∏£', '‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á', '‡πÄ‡∏û‡∏¥‡πà‡∏°', '‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç']):
                    sentiment = "negative"
                    confidence = 0.6
                elif any(word in text_lower for word in ['‡πÑ‡∏°‡πà‡∏°‡∏µ', '‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà', '‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö', '-']):
                    sentiment = "neutral"
                    confidence = 0.7
                else:
                    sentiment = "neutral"
                    confidence = 0.5
            else:
                if positive_count > negative_count:
                    sentiment = "positive"
                    confidence = min(0.9, 0.7 + (positive_count - negative_count) * 0.1)
                elif negative_count > positive_count:
                    sentiment = "negative"
                    confidence = min(0.9, 0.7 + (negative_count - positive_count) * 0.1)
                else:
                    sentiment = "neutral"
                    confidence = 0.6
            
            return {
                "sentiment": sentiment,
                "confidence": confidence,
                "positive_indicators": positive_count,
                "negative_indicators": negative_count,
                "neutral_indicators": neutral_count,
                "positive_words": positive_words,
                "negative_words": negative_words,
                "method": "rule_based"
            }
            
        except Exception as e:
            print(f"Error in sentiment analysis: {e}")
            return {
                "sentiment": "neutral",
                "confidence": 0.5,
                "positive_indicators": 0,
                "negative_indicators": 0,
                "neutral_indicators": 0,
                "positive_words": [],
                "negative_words": [],
                "method": "error"
            }
    
    def _extract_keywords_advanced(self, text: str) -> List[Dict]:
        """‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
        try:
            tokens = self._tokenize_thai(text)
            if not tokens:
                return []
            
            # ‡πÉ‡∏ä‡πâ POS tagging ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ PyThaiNLP
            if PYTHAINLP_AVAILABLE:
                try:
                    pos_tags = pos_tag(tokens)
                except:
                    pos_tags = [(token, "UNKNOWN") for token in tokens]
            else:
                pos_tags = [(token, "UNKNOWN") for token in tokens]
            
            keywords = []
            
            for word, pos in pos_tags:
                word_clean = word.strip().lower()
                
                if (len(word_clean) > 1 and 
                    not word_clean.isdigit() and
                    word_clean not in {'-', '/', '(', ')', '[', ']', '‡πÅ‡∏•‡∏∞', '‡∏ó‡∏µ‡πà', '‡∏Å‡∏≤‡∏£', '‡πÉ‡∏ô', '‡∏Ç‡∏≠‡∏á', '‡πÄ‡∏õ‡πá‡∏ô', '‡∏°‡∏µ', '‡πÑ‡∏î‡πâ', '‡∏à‡∏∞', '‡πÑ‡∏ß‡πâ', '‡∏ô‡∏µ‡πâ', '‡∏ô‡∏±‡πâ‡∏ô'}):
                    
                    base_score = 0.3
                    
                    if pos in ['ADJ', 'VERB']:
                        base_score += 0.3
                    elif pos in ['NOUN', 'ADV']:
                        base_score += 0.2
                    
                    sentiment_type = "neutral"
                    category = "general"
                    
                    if word_clean in self.positive_indicators:
                        base_score += 0.3
                        sentiment_type = "positive"
                    elif word_clean in self.negative_indicators:
                        base_score += 0.3
                        sentiment_type = "negative"
                    elif word_clean in self.neutral_indicators:
                        base_score += 0.1
                        sentiment_type = "neutral"
                    
                    for cat, keywords_list in self.survey_keywords.items():
                        if any(kw in word_clean for kw in keywords_list):
                            category = cat
                            base_score += 0.2
                            break
                    
                    keywords.append({
                        "word": word_clean,
                        "score": min(base_score, 1.0),
                        "frequency": 1,
                        "pos_tag": pos,
                        "sentiment_type": sentiment_type,
                        "category": category
                    })
            
            keywords.sort(key=lambda x: x["score"], reverse=True)
            return keywords[:8]
            
        except Exception as e:
            print(f"Error extracting keywords: {e}")
            words = text.split()
            return [{"word": word.lower(), "score": 0.5, "frequency": 1, 
                    "pos_tag": "UNKNOWN", "sentiment_type": "neutral", "category": "general"} 
                   for word in words[:3] if len(word) > 1]
    
    async def analyze_single_text(self, text: str, column_name: str = "") -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß"""
        if not text or not text.strip():
            return {
                "sentiment": "neutral",
                "confidence": 0.0,
                "keywords": [],
                "text": text,
                "column": column_name
            }
        
        try:
            processed_text = self._preprocess_text(text)
            
            if not processed_text:
                return {
                    "sentiment": "neutral",
                    "confidence": 0.0,
                    "keywords": [],
                    "text": text,
                    "column": column_name
                }
            
            sentiment_result = self._analyze_sentiment_rule_based(processed_text)
            keywords = self._extract_keywords_advanced(processed_text)
            
            return {
                "sentiment": sentiment_result["sentiment"],
                "confidence": sentiment_result["confidence"],
                "keywords": keywords,
                "text": text,
                "column": column_name,
                "debug": {
                    "processed_text": processed_text,
                    "positive_indicators": sentiment_result.get("positive_indicators", 0),
                    "negative_indicators": sentiment_result.get("negative_indicators", 0),
                    "positive_words": sentiment_result.get("positive_words", []),
                    "negative_words": sentiment_result.get("negative_words", []),
                    "method": sentiment_result.get("method", "unknown")
                }
            }
            
        except Exception as e:
            print(f"Error analyzing text: {e}")
            return {
                "sentiment": "neutral",
                "confidence": 0.0,
                "keywords": [],
                "text": text,
                "column": column_name
            }
    
    def _identify_survey_text_columns(self, df: pd.DataFrame) -> Dict[int, str]:
        """‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ö‡∏ö‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏° Post-Test Survey ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥"""
        
        column_mapping = {}
        
        for i, col_name in enumerate(df.columns):
            col_str = str(col_name).lower()
            
            # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô demographic ‡∏´‡∏£‡∏∑‡∏≠ metadata
            if any(skip_word in col_str for skip_word in ['timestamp', '‡πÄ‡∏ß‡∏•‡∏≤', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á', 'id', '‡∏•‡∏≥‡∏î‡∏±‡∏ö', '‡∏ä‡∏∑‡πà‡∏≠', '‡∏≠‡∏≤‡∏¢‡∏∏', '‡πÄ‡∏û‡∏®', 'email']):
                continue
            
            # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Yes/No ‡∏´‡∏£‡∏∑‡∏≠ ‡∏™‡∏ô‡πÉ‡∏à/‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à
            sample_data = df.iloc[:, i].dropna().astype(str).head(20).tolist()
            unique_values = set([str(val).strip() for val in sample_data])
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡∏™‡∏ô‡πÉ‡∏à/‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à
            if (len(unique_values) <= 3 and 
                any(val in ['‡∏™‡∏ô‡πÉ‡∏à', '‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à', '‡∏¢‡∏¥‡∏ô‡∏¢‡∏≠‡∏°', '‡πÑ‡∏°‡πà‡∏¢‡∏¥‡∏ô‡∏¢‡∏≠‡∏°'] for val in unique_values)):
                print(f"üö´ ‡∏Ç‡πâ‡∏≤‡∏° column {i+1} ({col_name}): ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")
                continue
                
            if not sample_data:
                continue
                
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            text_count = 0
            
            for text in sample_data:
                text_clean = text.strip()
                
                # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô
                if (len(text_clean) <= 2 or 
                    text_clean in ['-', '‡πÑ‡∏°‡πà‡∏°‡∏µ', '‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà', '‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö', '‡∏¢‡∏¥‡∏ô‡∏¢‡∏≠‡∏°', '‡πÑ‡∏°‡πà‡∏¢‡∏¥‡∏ô‡∏¢‡∏≠‡∏°', '‡∏™‡∏ô‡πÉ‡∏à', '‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à'] or
                    text_clean.isdigit() or
                    text_clean in ['1', '2', '3', '4', '5'] or
                    text_clean in ['1.0', '2.0', '3.0', '4.0', '5.0']):
                    continue
                    
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (choice) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                if any(choice in text_clean for choice in [
                    '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏£‡∏≤‡∏™‡∏≤‡∏£‡∏´‡∏ô‡∏µ‡πâ', '‡∏Ç‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£', '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡πÄ‡∏á‡∏¥‡∏ô',
                    'Email', 'OTP', 'ThaiD', '‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô'
                ]):
                    continue
                    
                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô
                if (any('\u0e00' <= char <= '\u0e7f' for char in text_clean) and 
                    len(text_clean) > 3):
                    text_count += 1
                    
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 30% ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            if text_count >= max(1, len(sample_data) * 0.3):
                # ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡∏°‡πà
                if '‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•' in col_str and '‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö' in col_str:
                    column_mapping[i] = "‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö"
                elif '‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ' in col_str or '‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î' in col_str:
                    column_mapping[i] = "‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"
                elif '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°' in col_str or '‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°' in col_str:
                    column_mapping[i] = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"
                elif '‡∏™‡∏±‡∏ö‡∏™‡∏ô' in col_str and '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà' in col_str and '‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•' in col_str:
                    column_mapping[i] = "‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"
                elif '‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á' in col_str or '‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç' in col_str:
                    column_mapping[i] = "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á"
                elif '‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥' in col_str or '‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°' in col_str:
                    column_mapping[i] = "‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"
                else:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÜ
                    print(f"ü§î ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö column {i+1} ({col_name}): {text_count}/{len(sample_data)} texts")
                    if text_count >= 3:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
                        column_mapping[i] = f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô_{str(col_name)}"
        
        print(f"üîç ‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {len(column_mapping)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
        for col_idx, col_name in column_mapping.items():
            print(f"  - ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå {col_idx + 1}: {col_name}")
            
        return column_mapping
    
    def _analyze_likert_scales(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Likert Scale ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"""
        likert_analysis = {}
        
        for i, col_name in enumerate(df.columns):
            col_str = str(col_name).lower()
            
            # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà likert scale
            if any(skip_word in col_str for skip_word in ['timestamp', '‡πÄ‡∏ß‡∏•‡∏≤', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'id', '‡∏•‡∏≥‡∏î‡∏±‡∏ö', 'email', '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡∏ä‡∏∑‡πà‡∏≠']):
                continue
                
            try:
                col_data = df.iloc[:, i]
                numeric_data = pd.to_numeric(col_data, errors='coerce').dropna()
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô likert scale (‡∏Ñ‡πà‡∏≤ 1-5) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                unique_values = set(numeric_data)
                is_likert = (len(unique_values) <= 5 and 
                           all(val in range(1, 6) for val in unique_values) and
                           len(numeric_data) > 0)
                
                if is_likert:
                    valid_scores = numeric_data[(numeric_data >= 1) & (numeric_data <= 5)]
                    
                    if len(valid_scores) > 0:
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ
                        if '‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô' in col_str or 'register' in col_str:
                            description = "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"
                        elif '‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°' in col_str or 'overall' in col_str:
                            description = "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°"
                        elif '‡πÄ‡∏ß‡∏•‡∏≤' in col_str or 'time' in col_str:
                            description = "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤"
                        elif '‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à' in col_str or 'confidence' in col_str:
                            description = "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à"
                        elif '‡∏û‡∏≠‡πÉ‡∏à' in col_str or 'satisfaction' in col_str:
                            description = "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏≠‡πÉ‡∏à"
                        else:
                            description = f"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô_{str(col_name)}"
                            
                        likert_analysis[description] = {
                            "mean": float(valid_scores.mean()),
                            "std": float(valid_scores.std()) if len(valid_scores) > 1 else 0.0,
                            "distribution": valid_scores.value_counts().sort_index().to_dict(),
                            "count": len(valid_scores),
                            "original_column": i,
                            "column_name": str(col_name)
                        }
                        
            except Exception as e:
                print(f"Could not analyze column {i} ({col_name}): {e}")
                continue
        
        return likert_analysis
    
    def _analyze_choice_questions(self, df: pd.DataFrame) -> Dict[str, Dict[str, int]]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≠‡∏ö"""
        choice_analysis = {}
        
        try:
            for i, col_name in enumerate(df.columns):
                col_str = str(col_name).lower()
                
                # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
                if any(skip_word in col_str for skip_word in ['timestamp', '‡πÄ‡∏ß‡∏•‡∏≤', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'id', '‡∏•‡∏≥‡∏î‡∏±‡∏ö', '‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•']):
                    continue
                
                # ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
                col_data = df.iloc[:, i].dropna()
                if len(col_data) == 0:
                    continue
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                value_counts = col_data.value_counts()
                unique_values = set(col_data.astype(str))
                
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡πÜ ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà likert scale)
                is_choice = (len(unique_values) <= 10 and 
                           len(value_counts) > 1 and
                           not all(str(val).isdigit() and 1 <= int(val) <= 5 for val in unique_values if str(val).isdigit()))
                
                if is_choice:
                    if '‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î' in col_str or 'best' in col_str:
                        choice_analysis["‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"] = value_counts.to_dict()
                    elif '‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö' in col_str or 'login' in col_str:
                        choice_analysis["‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö"] = value_counts.to_dict()
                    elif '‡∏™‡∏±‡∏ö‡∏™‡∏ô' in col_str and '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà' in col_str and '‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•' not in col_str:
                        choice_analysis["‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏ö‡∏™‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"] = value_counts.to_dict()
                    elif ('‡∏ó‡∏î‡∏™‡∏≠‡∏ö' in col_str and '‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á' in col_str) or ('‡∏™‡∏ô‡πÉ‡∏à' in col_str):
                        choice_analysis["‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"] = value_counts.to_dict()
                    else:
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô choice questions ‡∏à‡∏£‡∏¥‡∏á‡πÜ
                        choice_keywords = ['‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡∏Ç‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£', 'Email', 'OTP', 'ThaiD']
                        if any(any(keyword in str(val) for keyword in choice_keywords) for val in unique_values):
                            choice_analysis[f"‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≠‡∏ö_{str(col_name)}"] = value_counts.to_dict()
                        
        except Exception as e:
            print(f"Error analyzing choice questions: {e}")
        
        return choice_analysis
    
    async def generate_ai_insights(self, sentiment_dist: Dict, column_analysis: Dict, 
                                 keywords: List, detailed_results: List, 
                                 likert_analysis: Dict, choice_analysis: Dict) -> Dict:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á AI Insights ‡∏î‡πâ‡∏ß‡∏¢ Gemini API"""
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ AI
        summary_data = {
            "sentiment_summary": sentiment_dist,
            "top_keywords": [kw["word"] for kw in keywords[:10]],
            "negative_feedback_samples": [
                result["text"] for result in detailed_results 
                if result["sentiment"] == "negative"
            ][:5],
            "positive_feedback_samples": [
                result["text"] for result in detailed_results 
                if result["sentiment"] == "positive"
            ][:5],
            "likert_scores": {k: v.get("mean", 0) for k, v in likert_analysis.items()},
            "choice_results": choice_analysis
        }
        
        # ‡πÉ‡∏ä‡πâ Gemini API ‡∏´‡∏£‡∏∑‡∏≠ fallback ‡πÄ‡∏õ‡πá‡∏ô rule-based
        try:
            # TODO: ‡πÄ‡∏û‡∏¥‡πà‡∏° Gemini API call ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
            ai_insights = await self._call_gemini_api(summary_data)
            return ai_insights
        except Exception as e:
            print(f"AI API failed, using fallback: {e}")
            return self._generate_survey_insights_fallback(
                sentiment_dist, column_analysis, keywords, detailed_results, 
                likert_analysis, choice_analysis
            )
    
    async def _call_gemini_api(self, data: Dict) -> Dict:
        """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Gemini API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á insights"""
        # TODO: implement Gemini API call
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ fallback
        raise Exception("Gemini API not implemented yet")
    
    def _generate_survey_insights_fallback(self, sentiment_dist: Dict, column_analysis: Dict, 
                                 keywords: List, detailed_results: List, 
                                 likert_analysis: Dict, choice_analysis: Dict) -> Dict:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á insights ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Post-Test Survey (Fallback)"""
        insights = {
            "positive_aspects": [],
            "negative_aspects": [],
            "recommendations": [],
            "system_strengths": [],
            "improvement_areas": [],
            "user_pain_points": []
        }
        
        total_analyzed = sum(sentiment_dist.values())
        if total_analyzed == 0:
            return insights
        
        positive_percent = (sentiment_dist["positive"] / total_analyzed) * 100
        negative_percent = (sentiment_dist["negative"] / total_analyzed) * 100
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å sentiment
        if positive_percent > 60:
            insights["positive_aspects"].append(f"‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à‡∏™‡∏π‡∏á ({positive_percent:.1f}%)")
        elif positive_percent < 40:
            insights["negative_aspects"].append(f"‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à‡∏ï‡πà‡∏≥ ({positive_percent:.1f}%)")
            
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å keywords
        positive_keywords = [kw["word"] for kw in keywords[:10] 
                           if kw.get("sentiment_type") == "positive"]
        negative_keywords = [kw["word"] for kw in keywords[:10] 
                           if kw.get("sentiment_type") == "negative"]
        
        if positive_keywords:
            insights["system_strengths"].append(f"‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á: {', '.join(positive_keywords[:5])}")
            
        if negative_keywords:
            insights["user_pain_points"].append(f"‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å: {', '.join(negative_keywords[:5])}")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å likert analysis
        if likert_analysis:
            high_scores = {k: v for k, v in likert_analysis.items() if v.get("mean", 0) >= 4.0}
            low_scores = {k: v for k, v in likert_analysis.items() if v.get("mean", 0) < 3.5}
            
            if high_scores:
                best_aspect = max(high_scores.items(), key=lambda x: x[1].get("mean", 0))
                insights["positive_aspects"].append(f"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {best_aspect[0]} ({best_aspect[1].get('mean', 0):.2f}/5)")
            
            if low_scores:
                worst_aspect = min(low_scores.items(), key=lambda x: x[1].get("mean", 0))
                insights["improvement_areas"].append(f"‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô: {worst_aspect[0]} ({worst_aspect[1].get('mean', 0):.2f}/5)")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å choice questions - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ô‡πÉ‡∏à/‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à
        if choice_analysis:
            for question, answers in choice_analysis.items():
                if "‡∏™‡∏ô‡πÉ‡∏à" in question:
                    total_respondents = sum(answers.values())
                    interested = answers.get("‡∏™‡∏ô‡πÉ‡∏à", 0)
                    if total_respondents > 0:
                        interest_rate = (interested / total_respondents) * 100
                        if interest_rate >= 70:
                            insights["positive_aspects"].append(f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏π‡∏á ({interest_rate:.1f}%)")
                        elif interest_rate < 50:
                            insights["negative_aspects"].append(f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≥ ({interest_rate:.1f}%)")
                elif "‡∏™‡∏±‡∏ö‡∏™‡∏ô" in question:
                    confused_count = sum(count for answer, count in answers.items() if "‡∏™‡∏±‡∏ö‡∏™‡∏ô" in str(answer))
                    total_count = sum(answers.values())
                    if confused_count > total_count * 0.3:
                        insights["negative_aspects"].append(f"‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏±‡∏ö‡∏™‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ({confused_count}/{total_count})")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å detailed results
        common_issues = []
        positive_themes = []
        
        for result in detailed_results:
            text_lower = result["text"].lower()
            
            # ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢
            if "‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢" in text_lower and ("‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠" in text_lower or "‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à" in text_lower):
                common_issues.append("‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢")
            if "‡∏õ‡∏∏‡πà‡∏°" in text_lower and ("edit" in text_lower or "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç" in text_lower or "‡∏´‡∏≤" in text_lower):
                common_issues.append("‡∏õ‡∏∏‡πà‡∏°‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏≤‡∏¢‡∏≤‡∏Å")
            if "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà" in text_lower and "‡∏™‡∏±‡∏ö‡∏™‡∏ô" in text_lower:
                common_issues.append("‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô")
            
            # ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢
            if result["sentiment"] == "positive":
                if "‡∏™‡∏∞‡∏î‡∏ß‡∏Å" in text_lower or "‡∏á‡πà‡∏≤‡∏¢" in text_lower:
                    positive_themes.append("‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡πÅ‡∏•‡∏∞‡∏á‡πà‡∏≤‡∏¢")
                if "‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£" in text_lower:
                    positive_themes.append("‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£")
                if "‡πÄ‡∏£‡πá‡∏ß" in text_lower or "‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß" in text_lower:
                    positive_themes.append("‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß")
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° insights
        unique_issues = list(set(common_issues))
        unique_themes = list(set(positive_themes))
        
        if unique_issues:
            insights["negative_aspects"].extend(unique_issues[:3])
        if unique_themes:
            insights["system_strengths"].extend(unique_themes[:3])
            
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞
        recommendations = []
        
        if "‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢" in unique_issues:
            recommendations.append("‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô")
        if "‡∏õ‡∏∏‡πà‡∏°‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏≤‡∏¢‡∏≤‡∏Å" in unique_issues:
            recommendations.append("‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏∏‡πà‡∏°‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô")
        if "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô" in unique_issues:
            recommendations.append("‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô ‡∏ß‡∏±‡∏ô-‡πÄ‡∏î‡∏∑‡∏≠‡∏ô-‡∏õ‡∏µ (‡∏û.‡∏®.)")
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        if negative_percent > positive_percent:
            recommendations.append("‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á UX/UI ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à")
        if likert_analysis and any(v.get("mean", 0) < 3.5 for v in likert_analysis.values()):
            recommendations.append("‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à")
        
        recommendations.extend([
            "‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô",
            "‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö help ‡πÅ‡∏•‡∏∞ FAQ"
        ])
        
        insights["recommendations"] = recommendations[:5]
        
        return insights
    
    async def analyze_survey(self, df: pd.DataFrame, analysis_id: str) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ö‡∏ö‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏° Post-Test Survey ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô"""
        print(f"üéØ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Post-Test Survey ID: {analysis_id}")
        print(f"üìä ‡πÉ‡∏ä‡πâ Enhanced Rule-Based NLP (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: {self.model_accuracy:.2%})")
        print(f"üìè ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {df.shape[0]} ‡πÅ‡∏ñ‡∏ß, {df.shape[1]} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
        
        results = {
            "total_responses": len(df),
            "texts_analyzed": 0,
            "sentiment_distribution": {"positive": 0, "neutral": 0, "negative": 0},
            "top_keywords": [],
            "detailed_results": [],
            "column_analysis": {},
            "model_info": {
                "accuracy": f"{self.model_accuracy:.2%}",
                "engine": "Enhanced Rule-Based NLP",
                "preprocessing": "Thai tokenization + Advanced sentiment rules",
                "features": "Survey-specific keyword analysis + POS enhancement",
                "version": "Post-Test Survey Optimized v2.1"
            },
            "insights": {}
        }
        
        # ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        text_columns = self._identify_survey_text_columns(df)
        print(f"üìù ‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {len(text_columns)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Likert scales
        likert_analysis = self._analyze_likert_scales(df)
        results["likert_analysis"] = likert_analysis
        print(f"üìè ‡∏û‡∏ö Likert scales: {len(likert_analysis)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Choice questions
        choice_analysis = self._analyze_choice_questions(df)
        results["choice_analysis"] = choice_analysis
        print(f"‚òëÔ∏è ‡∏û‡∏ö Choice questions: {len(choice_analysis)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        all_keywords = {}
        
        for col_idx, col_description in text_columns.items():
            print(f"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: {col_description}")
            
            column_results = {
                "total_texts": 0,
                "sentiment_dist": {"positive": 0, "neutral": 0, "negative": 0},
                "sample_texts": {"positive": [], "negative": [], "neutral": []},
                "keywords_by_category": {}
            }
            
            if col_idx < len(df.columns):
                texts = df.iloc[:, col_idx].dropna().astype(str).tolist()
                
                for text in texts:
                    text_clean = text.strip()
                    
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
                    if (not text_clean or 
                        len(text_clean) <= 2 or 
                        text_clean in ['-', '‡πÑ‡∏°‡πà‡∏°‡∏µ', '‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà', '‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö', '‡∏¢‡∏¥‡∏ô‡∏¢‡∏≠‡∏°', '‡πÑ‡∏°‡πà‡∏¢‡∏¥‡∏ô‡∏¢‡∏≠‡∏°', '‡∏™‡∏ô‡πÉ‡∏à', '‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à'] or
                        text_clean.isdigit()):
                        continue
                        
                    analysis = await self.analyze_single_text(text_clean, col_description)
                    
                    results["detailed_results"].append({
                        "column": col_description,
                        "text": text_clean,
                        **analysis
                    })
                    
                    sentiment = analysis["sentiment"]
                    if sentiment in results["sentiment_distribution"]:
                        results["sentiment_distribution"][sentiment] += 1
                        column_results["sentiment_dist"][sentiment] += 1
                    
                    column_results["total_texts"] += 1
                    
                    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                    if len(column_results["sample_texts"][sentiment]) < 3:
                        column_results["sample_texts"][sentiment].append(text_clean[:150])
                    
                    # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° keywords
                    for kw in analysis["keywords"]:
                        word = kw["word"]
                        category = kw.get("category", "general")
                        
                        if word not in all_keywords:
                            all_keywords[word] = {
                                "count": 0, 
                                "total_score": 0, 
                                "sentiment_type": kw.get("sentiment_type", "neutral"),
                                "category": category,
                                "columns": set()
                            }
                        
                        all_keywords[word]["count"] += 1
                        all_keywords[word]["total_score"] += kw["score"]
                        all_keywords[word]["columns"].add(col_description)
                        
                        if category not in column_results["keywords_by_category"]:
                            column_results["keywords_by_category"][category] = {}
                        if word not in column_results["keywords_by_category"][category]:
                            column_results["keywords_by_category"][category][word] = 0
                        column_results["keywords_by_category"][category][word] += 1
            
            if column_results["total_texts"] > 0:
                results["column_analysis"][col_description] = column_results
                print(f"  ‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡πâ‡∏ß: {column_results['total_texts']} ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°")
        
        results["texts_analyzed"] = len(results["detailed_results"])
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ keywords
        sorted_keywords = sorted(
            all_keywords.items(),
            key=lambda x: (x[1]["count"], x[1]["total_score"]),
            reverse=True
        )[:25]
        
        results["top_keywords"] = [
            {
                "word": word,
                "count": data["count"],
                "avg_score": data["total_score"] / data["count"],
                "sentiment_type": data["sentiment_type"],
                "category": data["category"],
                "appears_in_columns": len(data["columns"])
            }
            for word, data in sorted_keywords
        ]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á AI insights
        results["insights"] = await self.generate_ai_insights(
            results["sentiment_distribution"], 
            results["column_analysis"],
            results["top_keywords"],
            results["detailed_results"],
            likert_analysis,
            choice_analysis
        )
        
        print(f"‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô:")
        print(f"  üìä ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {results['texts_analyzed']}")
        print(f"  üìà Sentiment: +{results['sentiment_distribution']['positive']} ={results['sentiment_distribution']['neutral']} -{results['sentiment_distribution']['negative']}")
        print(f"  üîë Keywords: {len(results['top_keywords'])} ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç")
        print(f"  üìè Likert scales: {len(likert_analysis)} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
        print(f"  ‚òëÔ∏è Choice questions: {len(choice_analysis)} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
        
        return results
    
    def is_ready(self) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"""
        return self._ready